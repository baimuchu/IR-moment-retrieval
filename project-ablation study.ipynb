{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"EdYzrLhxMTlW","trusted":true},"outputs":[],"source":["#from google.colab import drive\n","#drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3UmQotmJMUrs","trusted":true},"outputs":[],"source":["!pip install python-terrier==0.10.0 nltk scikit-learn lightgbm xgboost fastrank"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sRDXt2WiBNih","trusted":true},"outputs":[],"source":["# !pip install --upgrade git+https://github.com/Georgetown-IR-Lab/OpenNIR"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EYhewvEjL-7Y","trusted":true},"outputs":[],"source":["import pandas as pd\n","import json\n","import pyterrier as pt\n","import random"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nKsBbMKjEepz","trusted":true},"outputs":[],"source":["#UNCOMMENT THE FOLLOWING LINE TO USE EITHER THE TVR DATASET OR THE QVH DATASET\n","\n","dataset_choice = \"TVR\"\n","# dataset_choice = \"QVH\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UruQwRMsOrNj","trusted":true},"outputs":[],"source":["# Paths to JSONL files on Colab\n","# if dataset_choice == \"TVR\":\n","#    jsonl_train_path = '/content/drive/MyDrive/IR/text_data/tvr_train_release.jsonl'\n","#    jsonl_val_path = '/content/drive/MyDrive/IR/text_data/tvr_val_release.jsonl'\n","#    subs_path = '/content/drive/MyDrive/IR/text_data/tvqa_preprocessed_subtitles.jsonl'\n","\n","# elif dataset_choice == \"QVH\":\n","#    jsonl_train_path = \"/content/drive/MyDrive/IR/text_data_QVH/highlight_train_release.jsonl\"\n","#    subs_path = \"/content/drive/MyDrive/IR/text_data_QVH/subs_train.jsonl\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W17JgCyFL-7Z","trusted":true},"outputs":[],"source":["# Paths to JSONL files\n","if dataset_choice == \"TVR\":\n","    jsonl_train_path = '/kaggle/input/d/boxzhu/tvretrieval/tvr_train_release.jsonl'\n","    jsonl_val_path = '/kaggle/input/d/boxzhu/tvretrieval/tvr_val_release.jsonl'\n","    subs_path = '/kaggle/input/d/boxzhu/tvretrieval/tvqa_preprocessed_subtitles.jsonl'\n","\n","elif dataset_choice == \"QVH\":\n","    jsonl_train_path = \"text_data_QVH/highlight_train_release.jsonl\"\n","    subs_path = \"text_data_QVH/subs_train.jsonl\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v7uUCAs1xV6m","trusted":true},"outputs":[],"source":["# #Paths to JSONL files\n","# if dataset_choice == \"TVR\":\n","#     jsonl_train_path = 'text_data/tvr_train_release.jsonl'\n","#     jsonl_val_path = 'text_data/tvr_val_release.jsonl'\n","#     subs_path = 'text_data/tvqa_preprocessed_subtitles.jsonl'\n","\n","# elif dataset_choice == \"QVH\":\n","#     jsonl_train_path = \"text_data_QVH/highlight_train_release.jsonl\"\n","#     subs_path = \"text_data_QVH/subs_train.jsonl\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L6VZMDuDL-7Z","trusted":true},"outputs":[],"source":["# Load subtitles into a dictionary for quick access\n","# Load subtitles into a dictionary for quick access\n","subtitles_dict = {}\n","if dataset_choice == \"TVR\":\n","    with open(subs_path, 'r') as subs_file:\n","        for line in subs_file:\n","            sub_data = json.loads(line)\n","            subtitles_dict[sub_data['vid_name']] = sub_data['sub']\n","elif dataset_choice == \"QVH\":\n","    with open(subs_path, 'r') as subs_file:\n","        for line in subs_file:\n","            sub_data = json.loads(line)\n","            triple = sub_data['vid'].split(\"_\")\n","            name = sub_data['vid']\n","            #turn the list name into a string\n","            name = \"\".join(name)\n","            if name not in subtitles_dict:\n","                subtitles_dict[name] = [(float(triple[-2]) + sub_data[\"relevant_windows\"][0][0], float(triple[-2]) + sub_data[\"relevant_windows\"][0][1], sub_data['query'])]\n","            else:\n","                subtitles_dict[name].append((float(triple[-2]) + sub_data[\"relevant_windows\"][0][0], float(triple[-2]) + sub_data[\"relevant_windows\"][0][1], sub_data['query']))\n","\n","            # print(subtitles_dict[name])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UkWQOjjYL-7Z","trusted":true},"outputs":[],"source":["# Function to find matching subtitles in TVR case\n","def find_matching_subtitles(vid_name, ts_range, subtitles_dict):\n","    matching_subs = []\n","    if vid_name in subtitles_dict:\n","        for subtitle in subtitles_dict[vid_name]:\n","            if (ts_range[0] <= subtitle['start'] <= ts_range[1]) or (ts_range[0] <= subtitle['end'] <= ts_range[1]) or (subtitle['start'] <= ts_range[0] and subtitle['end'] >= ts_range[1]):\n","                matching_subs.append(subtitle['text'])\n","    return matching_subs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HDx8ntjxL-7a","trusted":true},"outputs":[],"source":["def parse_jsonl_TVR(jsonl_path, split_type):\n","    # Initialize empty lists for your data\n","    queries_data = []\n","    documents_data = []\n","    query_rankings_data = []\n","\n","    with open(jsonl_path, 'r') as file:\n","        for idx, line in enumerate(file):\n","            data = json.loads(line)\n","            # drop non text-based queries\n","            if data['type'] not in ['t']:\n","                continue\n","\n","            # Find matching subtitles\n","            matching_subs = find_matching_subtitles(data['vid_name'], data['ts'], subtitles_dict)\n","\n","            if matching_subs == []:\n","                continue\n","\n","            # Extract data for the Query Set DataFrame\n","            queries_data.append({'qid': str(data['desc_id']), 'query': data['desc']})\n","\n","            # Extract data for the Documents Set DataFrame, including matching subtitles\n","            documents_data.append({'docno': split_type + str(idx), 'vid_name': data['vid_name'], 'ts': data['ts'],\n","                                'duration': data['duration'], 'type': data['type'], 'text': \"\".join(matching_subs)})\n","\n","            # Extract data for the Query Rankings DataFrame\n","            query_rankings_data.append({'qid': str(data[\"desc_id\"]), 'query': data['desc'], 'docno': split_type + str(idx), 'rank': 1, 'score': 1.0})\n","\n","    return queries_data, documents_data, query_rankings_data\n","\n","def parse_jsonl_QVH(jsonl_path):\n","    queries_data = []\n","    documents_data = []\n","    query_rankings_data = []\n","    with open(jsonl_path, 'r') as file:\n","        for idx,line in enumerate(file):\n","\n","            # Load the JSON object from the line\n","            data = json.loads(line)\n","\n","            triple = data[\"vid\"].split(\"_\")\n","            document_name = triple[0:-2]\n","            document_name = data[\"vid\"]\n","            start_time = float(triple[-2])\n","            end_time = float(triple[-1])\n","\n","            if document_name not in subtitles_dict:\n","                #print(\"Document not found in subtitles: \", document_name)\n","                continue\n","\n","            subs = []\n","            for relevant_window in data[\"relevant_windows\"]:\n","                ts = [start_time+relevant_window[0], start_time+relevant_window[1]]\n","                subs.extend([sub for sub in subtitles_dict[document_name] if sub[0] <= ts[1] and ts[0] <= sub[1]])\n","                if len(subs) == 0:\n","                    #print(\"No subtitles found for \", document_name, \" at time \", ts)\n","                    continue\n","\n","            documents_data.append({\"docno\": str(idx), \"vid_name\": document_name, \"ts\": [start_time, end_time], \"duration\" : data[\"duration\"], \"text\": \" \".join([sub[2] for sub in subs])})\n","            queries_data.append({\"qid\" : str(data[\"qid\"]), \"query\": data[\"query\"]})\n","            query_rankings_data.append({\"qid\": str(data[\"qid\"]), \"query\": data[\"query\"], \"docno\": str(idx), \"rank\": 1, \"score\": 1.0})\n","\n","    return queries_data, documents_data, query_rankings_data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ae33k1XsEep2","trusted":true},"outputs":[],"source":["\n","if dataset_choice == \"TVR\":\n","    queries_data_train, documents_data_train, query_rankings_data_train = parse_jsonl_TVR(jsonl_train_path, \"t\")\n","    queries_data_val, documents_data_val, query_rankings_data_val = parse_jsonl_TVR(jsonl_val_path, \"v\")\n","    #have to create a test set; to do it, extract a random 10% of the train set\n","    random.seed(42)\n","    query_rankings_data_test = random.sample(query_rankings_data_train, int(len(query_rankings_data_train)*0.1))\n","    query_rankings_data_train = [query for query in query_rankings_data_train if query not in query_rankings_data_test]\n","    queries_data_test = [query for query in queries_data_train if query[\"qid\"] in [query[\"qid\"] for query in query_rankings_data_test]]\n","    queries_data_train = [query for query in queries_data_train if query not in queries_data_test]\n","    documents_data_test = [doc for doc in documents_data_train if doc[\"docno\"] in [query[\"docno\"] for query in query_rankings_data_test]]\n","    documents_data_train = [doc for doc in documents_data_train if doc not in documents_data_test]\n","\n","elif dataset_choice == \"QVH\":\n","    queries_data_train, documents_data_train, query_rankings_data_train = parse_jsonl_QVH(jsonl_train_path)\n","    #have to create a val set; to do it, extract a random 10% of the train set\n","    random.seed(42)\n","    query_rankings_data_val = random.sample(query_rankings_data_train, int(len(query_rankings_data_train)*0.1))\n","    query_rankings_data_train = [query for query in query_rankings_data_train if query not in query_rankings_data_val]\n","    queries_data_val = [query for query in queries_data_train if query[\"qid\"] in [query[\"qid\"] for query in query_rankings_data_val]]\n","    queries_data_train = [query for query in queries_data_train if query not in queries_data_val]\n","    documents_data_val = [doc for doc in documents_data_train if doc[\"docno\"] in [query[\"docno\"] for query in query_rankings_data_val]]\n","    documents_data_train = [doc for doc in documents_data_train if doc not in documents_data_val]\n","    #have to create a test set; to do it, extract a random 10% of the train set\n","    random.seed(42)\n","    query_rankings_data_test = random.sample(query_rankings_data_train, int(len(query_rankings_data_train)*0.1))\n","    query_rankings_data_train = [query for query in query_rankings_data_train if query not in query_rankings_data_test]\n","    queries_data_test = [query for query in queries_data_train if query[\"qid\"] in [query[\"qid\"] for query in query_rankings_data_test]]\n","    queries_data_train = [query for query in queries_data_train if query not in queries_data_test]\n","    documents_data_test = [doc for doc in documents_data_train if doc[\"docno\"] in [query[\"docno\"] for query in query_rankings_data_test]]\n","    documents_data_train = [doc for doc in documents_data_train if doc not in documents_data_test]\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZTiDgmaqL-7a","outputId":"b7e64317-7faf-47c4-e8a9-20be5a030ea7","trusted":true},"outputs":[],"source":["# Create DataFrames for the Query Set, Documents Set, and Query Rankings\n","queries_train_df = pd.DataFrame(queries_data_train)\n","documents_train_df = pd.DataFrame(documents_data_train)\n","\n","queries_val_df = pd.DataFrame(queries_data_val)\n","documents_val_df = pd.DataFrame(documents_data_val)\n","\n","queries_test_df = pd.DataFrame(queries_data_test)\n","documents_test_df = pd.DataFrame(documents_data_test)\n","\n","q_rels = pd.concat([pd.DataFrame(query_rankings_data_train), pd.DataFrame(query_rankings_data_val), pd.DataFrame(query_rankings_data_test)]).reset_index(drop=True)\n","\n","#print length of the dataframes\n","print(\"Train set:\")\n","print(\"Queries: \", len(queries_train_df))\n","print(\"Documents: \", len(documents_train_df))\n","\n","print(\"Val set:\")\n","print(\"Queries: \", len(queries_val_df))\n","print(\"Documents: \", len(documents_val_df))\n","\n","print(\"Test set:\")\n","print(\"Queries: \", len(queries_test_df))\n","print(\"Documents: \", len(documents_test_df))\n","\n","print(\"Query Rankings: \", len(q_rels))\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NCaV5vcXL-7a"},"source":["### First Stage Retrieval [TODO: BOX]\n","The following part of the code will define three different first stage retrieval pipelines as an input for the trained model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5-SsgPJwVMXw","outputId":"0f9acf41-25d4-4a44-b46e-f5b34b398b2d","trusted":true},"outputs":[],"source":["!ls -la /usr/lib/jvm/java-11-openjdk-amd64\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n","\n","if not pt.started():\n","    pt.init()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xx55BjIGfNNa","trusted":true},"outputs":[],"source":["# Create an index\n","from pathlib import Path\n","\n","indexer = pt.IterDictIndexer(\n","    \"./index_path/\",\n","    meta={\n","        \"docno\": 64,\n","        \"vid_name\": 64,\n","        \"text\": 131072,\n","    },\n","    stemmer=\"porter\",\n","    stopwords=\"terrier\",\n","    overwrite=True,\n","    type=pt.index.IndexingType.MEMORY,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T1W-_BFDjpqS","outputId":"657431e1-1296-4391-9ac1-113827b51bcd","trusted":true},"outputs":[],"source":["joint_documents_set_df = pd.concat([documents_train_df, documents_val_df, documents_test_df])\n","\n","print(\"Length: \", len(joint_documents_set_df))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R_PnpogfjpqS","trusted":true},"outputs":[],"source":["indexed = indexer.index(\n","    joint_documents_set_df.to_dict(orient=\"records\")\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9xuAPswuVQMe","trusted":true},"outputs":[],"source":["#These are some examples, not necessarily the best ones. Experiment with different models\n","\n","# Initialize BatchRetrieve with the created index and specify BM25 as the weighting model\n","first_stage_bm25 = pt.BatchRetrieve(\n","    indexed,\n","    wmodel=\"BM25\",\n","    num_results=100,\n","    metadata=[\"docno\", \"vid_name\", \"text\"]\n",")\n","\n","# Initialize BatchRetrieve with the created index and specify LemurTF_IDF as the weighting model\n","first_stage_lemurtfidf = pt.BatchRetrieve(\n","    indexed,\n","    wmodel=\"LemurTF_IDF\",\n","    num_results=100,\n","    metadata=[\"docno\", \"vid_name\", \"text\"]\n",")\n","\n","# Initialize BatchRetrieve with the created index and specify Hiemstra_LM as the weighting model\n","first_stage_hiemstra_lm = pt.BatchRetrieve(\n","    indexed,\n","    wmodel=\"Hiemstra_LM\",\n","    num_results=100,\n","    metadata=[\"docno\", \"vid_name\", \"text\"]\n",")\n","\n","first_stage_dfic = pt.BatchRetrieve(\n","    indexed,\n","    wmodel=\"DFIC\",\n","    num_results=100,\n","    metadata=['docno', 'vid_name', 'text']\n",")\n","\n","first_stage_lgd = pt.BatchRetrieve(\n","    indexed,\n","    wmodel=\"LGD\",\n","    num_results=100,\n","    metadata=[\"docno\", \"vid_name\", \"text\"]\n",")\n","\n","first_stage_in_exp_b2 = pt.BatchRetrieve(\n","    indexed,\n","    wmodel=\"In_expB2\",\n","    num_results=100,\n","    metadata=[\"docno\", \"vid_name\", \"text\"]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# RDM\n","import numpy as np\n","import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","vectorizer = TfidfVectorizer()\n","tfidf_matrix = vectorizer.fit_transform(subtitles['prepared_trainqueries'])\n","\n","def enhance_query_with_rdm(query):\n","    query_vec = vectorizer.transform([query])\n","    cos_similarities = cosine_similarity(query_vec, tfidf_matrix).flatten()\n","    # Select top-N most similar subtitles to enhance the query representation\n","    top_n_idx = np.argsort(cos_similarities)[-5:]  # For example, top 5\n","    top_n_texts = subtitles.iloc[top_n_idx]['prepared_trainqueries'].values\n","    enhanced_query = ' '.join([query] + list(top_n_texts))\n","    return enhanced_query\n","\n","queries['prepared_trainqueries'] = queries['prepared_trainqueries'].apply(enhance_query_with_rdm)\n"]},{"cell_type":"markdown","metadata":{"id":"7vCeCMoUvedO"},"source":["# Computing feature\n","The weighting model can be use in [pt.weighting_model.package: http://terrier.org/docs/current/javadoc/org/terrier/matching/models/package-summary.html](http://terrier.org/docs/current/javadoc/org/terrier/matching/models/package-summary.html)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QTBZtHYLy_fH","trusted":true},"outputs":[],"source":["#We create features for the second stage using the first stage retrievers\n","\n","#TF-IDF based features\n","lemur_tf_idf_retriever = pt.BatchRetrieve(indexed, wmodel=\"LemurTF_IDF\")\n","bm25_retriever = pt.BatchRetrieve(indexed, wmodel=\"BM25\")\n","tf_idf_retriever = pt.BatchRetrieve(indexed, wmodel=\"TF_IDF\")\n","\n","#Language model based features\n","hiem_retriever = pt.BatchRetrieve(indexed, wmodel=\"Hiemstra_LM\")\n","dirichlet_retriever = pt.BatchRetrieve(indexed, wmodel=\"DirichletLM\")\n","\n","#Divergence from randomness based features\n","pl2_retriever = pt.BatchRetrieve(indexed, wmodel=\"PL2\")\n","dph_retriever = pt.BatchRetrieve(indexed, wmodel=\"DPH\")\n","dlh_retriever = pt.BatchRetrieve(indexed, wmodel=\"DLH\")\n","\n","#Can add more!\n","coordinate_match_retriever = pt.BatchRetrieve(indexed, wmodel=\"CoordinateMatch\")\n","js_kls_retrieveer = pt.BatchRetrieve(indexed, wmodel=\"Js_KLs\")\n","dfic_retriever = pt.BatchRetrieve(indexed, wmodel=\"DFIC\")\n","in_exp_b2_retriever = pt.BatchRetrieve(indexed, wmodel=\"In_expB2\")\n","lgd_retriever = pt.BatchRetrieve(indexed, wmodel=\"LGD\")\n","mdl2_retriever = pt.BatchRetrieve(indexed, wmodel=\"MDL2\")\n","ifp2_retrieveer = pt.BatchRetrieve(indexed, wmodel=\"IFB2\")\n","pl2f_retriever = pt.BatchRetrieve(indexed, wmodel=\"PL2F\")\n","SMF_retriever = pt.BatchRetrieve(indexed, wmodel=\"SingleFieldModel\")\n","static_retriever = pt.BatchRetrieve(indexed, wmodel=\"StaticFeature\")\n","xsqrA_retriever = pt.BatchRetrieve(indexed, wmodel=\"XSqrA_M\")\n","di_retriever = pt.BatchRetrieve(indexed, wmodel=\"Dl\")\n","dlh_retrieveer = pt.BatchRetrieve(indexed, wmodel=\"DLH\")\n","ifb2_retriever = pt.BatchRetrieve(indexed, wmodel=\"IFB2\")\n","in_exp_c2_retriever = pt.BatchRetrieve(indexed, wmodel=\"In_expC2\")\n","in_b2_retriever = pt.BatchRetrieve(indexed, wmodel=\"InB2\")\n","in_l2_retrieveer = pt.BatchRetrieve(indexed, wmodel=\"InL2\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AYrqwBx1vVn_","trusted":true},"outputs":[],"source":["# PIPELINES WITH FSR AND FEATURES\n","\n","#Can test many possibilities, i just put some examples here\n","\n","in_exp_b2_pipeline = ~first_stage_in_exp_b2 >> (\n","    bm25_retriever ** hiem_retriever ** pl2_retriever ** coordinate_match_retriever ** in_exp_b2_retriever\n",")\n","\n","in_exp_b2_pipeline_woBM25 = ~first_stage_in_exp_b2 >> (\n","    hiem_retriever ** pl2_retriever ** coordinate_match_retriever ** in_exp_b2_retriever\n",")\n","\n","in_exp_b2_pipeline_wohiem = ~first_stage_in_exp_b2 >> (\n","    bm25_retriever ** pl2_retriever ** coordinate_match_retriever ** in_exp_b2_retriever\n",")\n","\n","in_exp_b2_pipeline_wocoodinate = ~first_stage_in_exp_b2 >> (\n","    bm25_retriever ** hiem_retriever ** pl2_retriever ** in_exp_b2_retriever\n",")\n","#NOTE: i believe we should not use the same first stage retriever for features AND for the first stage retrieval"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QE1WrfC8zJjm","trusted":true},"outputs":[],"source":["# Prepare the queries for the pipeline, remove special characters and extra spaces\n","prepared_trainqueries = queries_train_df\n","prepared_trainqueries['query'] = prepared_trainqueries['query'].str.replace('[\\'\"?!]', ' ', regex=True)\n","prepared_trainqueries['query'] = prepared_trainqueries['query'].str.replace(r'[^\\w\\s]', ' ', regex=True)\n","prepared_trainqueries['query'] = prepared_trainqueries['query'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n","\n","prepared_train_qrels = pd.DataFrame(query_rankings_data_train)\n","prepared_train_qrels['query'] = prepared_train_qrels['query'].str.replace('[\\'\"?!]', ' ', regex=True)\n","prepared_train_qrels['query'] = prepared_train_qrels['query'].str.replace(r'[^\\w\\s]', ' ', regex=True)\n","prepared_train_qrels['query'] = prepared_train_qrels['query'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n","\n","prepared_train_qrels['label'] = prepared_train_qrels['score']\n","\n","prepared_train_qrels['label'] = prepared_train_qrels['label'].astype(int)\n","\n","prepared_val_qrels = pd.DataFrame(query_rankings_data_val)\n","prepared_val_qrels['query'] = prepared_val_qrels['query'].str.replace('[\\'\"?!]', ' ', regex=True)\n","prepared_val_qrels['query'] = prepared_val_qrels['query'].str.replace(r'[^\\w\\s]', ' ', regex=True)\n","prepared_val_qrels['query'] = prepared_val_qrels['query'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n","\n","prepared_val_qrels['label'] = prepared_val_qrels['score']\n","\n","prepared_val_qrels['label'] = prepared_val_qrels['label'].astype(int)\n","\n","prepared_test_qrels = pd.DataFrame(query_rankings_data_test)\n","prepared_test_qrels['query'] = prepared_test_qrels['query'].str.replace('[\\'\"?!]', ' ', regex=True)\n","prepared_test_qrels['query'] = prepared_test_qrels['query'].str.replace(r'[^\\w\\s]', ' ', regex=True)\n","prepared_test_qrels['query'] = prepared_test_qrels['query'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n","\n","prepared_test_qrels['label'] = prepared_test_qrels['score']\n","\n","prepared_test_qrels['label'] = prepared_test_qrels['label'].astype(int)\n","\n","prepared_qrels = q_rels\n","prepared_qrels['query'] = prepared_qrels['query'].str.replace('[\\'\"?!]', ' ', regex=True)\n","prepared_qrels['query'] = prepared_qrels['query'].str.replace(r'[^\\w\\s]', ' ', regex=True)\n","prepared_qrels['query'] = prepared_qrels['query'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n","\n","prepared_qrels['label'] = prepared_qrels['score']\n","\n","prepared_qrels['label'] = prepared_qrels['label'].astype(int)\n","\n","prepared_valqueries = queries_val_df.reset_index()\n","prepared_valqueries['query'] = prepared_valqueries['query'].str.replace('[\\'\"?!]', ' ', regex=True)\n","prepared_valqueries['query'] = prepared_valqueries['query'].str.replace(r'[^\\w\\s]', ' ', regex=True)\n","\n","prepared_valqueries['query'] = prepared_valqueries['query'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n","\n","#test set\n","prepared_testqueries = queries_test_df.reset_index()\n","prepared_testqueries['query'] = prepared_testqueries['query'].str.replace('[\\'\"?!]', ' ', regex=True)\n","prepared_testqueries['query'] = prepared_testqueries['query'].str.replace(r'[^\\w\\s]', ' ', regex=True)\n","prepared_testqueries['query'] = prepared_testqueries['query'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"as3pBZo-xV6s","trusted":true},"outputs":[],"source":["# pt.Experiment(\n","#     names = [\"BM25\",\"Hiemstra\"],\n","#     retr_systems = [bm25_pipeline, hiem_lm_pipeline],\n","#     topics = prepared_valqueries,\n","#     qrels = prepared_val_qrels,\n","#     eval_metrics = [\"map\", \"ndcg_cut_10\", \"P_10\"],\n","# )"]},{"cell_type":"markdown","metadata":{"id":"xLeXGDj8rGcx"},"source":["Storage format: .csv or trec\n","\n","trec(https://pyterrier.readthedocs.io/en/latest/io.html): The pt io format, but it doesn't contain feature."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RGv0tMvLEep6","trusted":true},"outputs":[],"source":["from sklearn.svm import SVR\n","import xgboost as xgb\n","import fastrank\n","\n","# index = pt.IndexFactory.of(\"./index_path\")\n","\n","fsr_pipelines = [\n","    {\n","       'pipe': in_exp_b2_pipeline,\n","       'name': 'In Exp  B2'\n","    },\n","    {\n","       'pipe': in_exp_b2_pipeline_woBM25,\n","       'name': 'In Exp  B2_woBM25'\n","    },\n","    {\n","       'pipe': in_exp_b2_pipeline_wohiem,\n","       'name': 'In Exp  B2_wohiem'\n","    },\n","    {\n","       'pipe': in_exp_b2_pipeline_wocoodinate,\n","       'name': 'In Exp  B2_wocoodinate'\n","    },\n","    \n","]\n","\n","learned_models = [\n","#     {\n","#         'model': SVR(),\n","#         'form': 'reg',\n","#         'name': 'SVR',\n","#     },\n","#     {\n","#         'model': xgb.XGBRanker(tree_method=\"hist\", objective=\"rank:ndcg\"),\n","#         'form': 'ltr',\n","#         'name': 'XGBoost (NDCG)',\n","#     },\n","    #{\n","    #    'model': xgb.XGBRanker(tree_method=\"hist\", lambdarank_num_pair_per_sample=8, objective=\"rank:pairwise\", lambdarank_pair_method=\"topk\"),\n","    #    'form': 'ltr',\n","    #    'name': 'XGBoost (Pariwise)'\n","    #}\n","    # {\n","    #     'model': xgb.XGBRanker(tree_method=\"hist\", objective=\"rank:map\"),\n","    #     'form': 'ltr',\n","    #     'name': 'XGBoost (MAP)',\n","    # },\n","#     {\n","#        'model': fastrank.TrainRequest.coordinate_ascent(),\n","#        'form': 'fastrank',\n","#        'name': 'FastRank Coordinate Ascent',\n","#     },\n","    {\n","       'model': fastrank.TrainRequest.random_forest(),\n","       'form': 'fastrank',\n","       'name': 'FastRank Random Forest',\n","    }\n","  ]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# from pyterrier.measures import *\n","\n","# trained_models = [first_stage_bm25, first_stage_lemurtfidf, first_stage_hiemstra_lm, first_stage_dfic,  first_stage_lgd, first_stage_in_exp_b2]\n","# names = ['BM25', 'LemurTF_IDF', 'Hiemstra LM', 'DFIC', 'LGD', 'In Exp  B2']\n","# eval_metrics = [\n","#     nDCG @ 1, nDCG @ 3, nDCG @ 5, nDCG @ 10, nDCG @ 30,\n","#     RR @ 1,   RR @ 3,   RR @ 5,   RR @ 10, RR @ 30,\n","#     MAP, NumRelRet, Judged @ 1,Judged @ 3, Judged @ 5\n","# ]\n","\n","# for fsr in fsr_pipelines:\n","#     for model in learned_models:\n","#         names.append(f\"{fsr['name']} >> {model['name']}\")\n","#         print(names[-1])\n","#         if 'form' in model:\n","#           pipe = fsr['pipe'] >> pt.ltr.apply_learned_model(model['model'], form=model['form'])\n","#         else:\n","#           pipe = fsr['pipe'] >> pt.ltr.apply_learned_model(model['model'])\n","#         pipe.fit(\n","#             prepared_trainqueries,\n","#             prepared_train_qrels,\n","#             prepared_valqueries,\n","#             prepared_val_qrels\n","#         )\n","#         trained_models.append(pipe)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lOrma3FeEep6","trusted":true},"outputs":[],"source":["from pyterrier.measures import *\n","\n","trained_models = [first_stage_in_exp_b2]\n","names = ['In Exp  B2']\n","eval_metrics = [\n","    Recall @ 1,   Recall @ 3,    Recall @ 5,    Recall @ 10, \n","    Recall @ 20,  Recall @ 30,   Recall @ 50,   Recall @ 100,\n","]\n","\n","for fsr in fsr_pipelines:\n","    for model in learned_models:\n","        names.append(f\"{fsr['name']} >> {model['name']}\")\n","        print(names[-1])\n","        if 'form' in model:\n","          pipe = fsr['pipe'] >> pt.ltr.apply_learned_model(model['model'], form=model['form'])\n","        else:\n","          pipe = fsr['pipe'] >> pt.ltr.apply_learned_model(model['model'])\n","        pipe.fit(\n","            prepared_trainqueries,\n","            prepared_train_qrels,\n","            prepared_valqueries,\n","            prepared_val_qrels\n","        )\n","        trained_models.append(pipe)\n","        \n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from pyterrier.measures import nDCG, RR, MAP\n","\n","pt.Experiment(\n","    trained_models,\n","    prepared_testqueries,\n","    prepared_test_qrels,\n","    names=names,\n","    eval_metrics=eval_metrics,\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vhtfFvM0Eep6","outputId":"6b0eda5a-3fcd-45a8-dbeb-8c451dc077aa","trusted":true},"outputs":[],"source":["from pyterrier.measures import nDCG, RR, MAP\n","\n","pt.Experiment(\n","    trained_models,\n","    prepared_valqueries,\n","    prepared_val_qrels,\n","    names=names,\n","    eval_metrics=eval_metrics,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7C9lfwR5KPsP","outputId":"d92b774f-fcd1-401d-8e45-b861dbe12d8a","trusted":true},"outputs":[],"source":["from pyterrier.measures import nDCG, RR, MAP\n","\n","pt.Experiment(\n","    trained_models,\n","    prepared_trainqueries[:1000],\n","    prepared_train_qrels[:1000],\n","    names=names,\n","    eval_metrics=eval_metrics,\n",")"]}],"metadata":{"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4710295,"sourceId":7999171,"sourceType":"datasetVersion"}],"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
