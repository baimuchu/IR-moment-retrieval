{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import pyterrier as pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to JSONL files\n",
    "jsonl_path = \"text_data_QVH/highlight_train_release.jsonl\"\n",
    "subs_path = \"text_data_QVH/subs_train.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtitles_dict = {}\n",
    "\n",
    "with open(subs_path, 'r') as subs_file:\n",
    "    for line in subs_file:\n",
    "        sub_data = json.loads(line)\n",
    "        triple = sub_data['vid'].split(\"_\")\n",
    "        name = triple[0:-2]\n",
    "        #turn the list name into a string\n",
    "        name = \"\".join(name)\n",
    "        if name not in subtitles_dict:\n",
    "            subtitles_dict[name] = [(float(triple[-2]) + sub_data[\"relevant_windows\"][0][0], float(triple[-2]) + sub_data[\"relevant_windows\"][0][1], sub_data['query'])]\n",
    "        else:\n",
    "            subtitles_dict[name].append((float(triple[-2]) + sub_data[\"relevant_windows\"][0][0], float(triple[-2]) + sub_data[\"relevant_windows\"][0][1], sub_data['query']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_jsonl(jsonl_path):\n",
    "    queries_data = []\n",
    "    documents_data = []\n",
    "    query_rankings_data = []\n",
    "    with open(jsonl_path, 'r') as file:\n",
    "        for idx,line in enumerate(file):\n",
    "\n",
    "            # Load the JSON object from the line\n",
    "            data = json.loads(line)\n",
    "\n",
    "            triple = data[\"vid\"].split(\"_\")\n",
    "            document_name = triple[0:-2]\n",
    "            document_name = \"\".join(document_name)\n",
    "            start_time = float(triple[-2])\n",
    "            end_time = float(triple[-1])\n",
    "\n",
    "            if document_name not in subtitles_dict:\n",
    "                #print(\"Document not found in subtitles: \", document_name)\n",
    "                continue\n",
    "\n",
    "            queries_data.append({\"qid\" : data[\"qid\"], \"query\": data[\"query\"]})\n",
    "\n",
    "            all_scores = []\n",
    "            momentaneus_rank =[]\n",
    "            for id,relevant_window in enumerate(data[\"relevant_windows\"]):\n",
    "                ts = [start_time+relevant_window[0], start_time+relevant_window[1]]\n",
    "                subs = [sub for sub in subtitles_dict[document_name] if sub[0] <= ts[1] and ts[0] <= sub[1]]\n",
    "                documents_data.append({\"docno\" : str(idx) +\"_\"+str(ts[0]) + \"_\" + str(ts[1]), \"vid_name\" : document_name, \"ts\": ts, \"duration\": data[\"duration\"], \"text\": \"\".join([sub[2] for sub in subs])})\n",
    "                scores = [data[\"saliency_scores\"][i]  for i,clip_id in enumerate(data[\"relevant_clip_ids\"]) if clip_id*2 >= relevant_window[0] and clip_id*2 <= relevant_window[1]]\n",
    "                #each entry of scores is a triple of integers. Create a variable score which is the average of all the scores\n",
    "                score = 0 if len(scores) ==0 else sum(sum(scores[i]) for i in range(len(scores)))/(3*len(scores))\n",
    "\n",
    "                momentaneus_rank.append({\"qid\" : data[\"qid\"], \"query\": data[\"query\"] , \"docno\" : str(idx) +\"_\"+str(ts[0]) + \"_\" + str(ts[1]), \"score\": score, \"rank\":1})\n",
    "            \n",
    "            #adjust the rank of the momentaneus_rank based on the score\n",
    "            momentaneus_rank = sorted(momentaneus_rank, key=lambda x: x[\"score\"], reverse=True)\n",
    "            for i in range(len(momentaneus_rank)):\n",
    "                momentaneus_rank[i][\"rank\"] = i+1\n",
    "        \n",
    "            query_rankings_data.extend(momentaneus_rank)\n",
    "\n",
    "    return queries_data, documents_data, query_rankings_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_data, documents_data, query_rankings_data = parse_jsonl(jsonl_path)\n",
    "\n",
    "# Convert lists to DataFrames (training)\n",
    "query_set_df = pd.DataFrame(queries_data)\n",
    "documents_set_df = pd.DataFrame(documents_data)\n",
    "query_rankings_df = pd.DataFrame(query_rankings_data)\n",
    "\n",
    "train_query_set_df = query_set_df.sample(frac=0.8)\n",
    "val_query_set_df = query_set_df.drop(train_query_set_df.index)\n",
    "\n",
    "train_query_rankings_df = query_rankings_df[query_rankings_df[\"qid\"].isin(train_query_set_df[\"qid\"])]\n",
    "val_query_rankings_df = query_rankings_df[query_rankings_df[\"qid\"].isin(val_query_set_df[\"qid\"])]\n",
    "\n",
    "train_documents_set_df = documents_set_df[documents_set_df[\"docno\"].isin(train_query_rankings_df[\"docno\"])]\n",
    "val_documents_set_df = documents_set_df[documents_set_df[\"docno\"].isin(val_query_rankings_df[\"docno\"])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4445\n",
      "7942\n",
      "7942\n",
      "1111\n",
      "1940\n",
      "1940\n"
     ]
    }
   ],
   "source": [
    "#print the length of the dataframes\n",
    "print(len(train_query_set_df))\n",
    "print(len(train_documents_set_df))\n",
    "print(len(train_query_rankings_df))\n",
    "\n",
    "print(len(val_query_set_df))\n",
    "print(len(val_documents_set_df))\n",
    "print(len(val_query_rankings_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.10.0 has loaded Terrier 5.8 (built by craigm on 2023-11-01 18:05) and terrier-helper 0.0.8\n",
      "\n",
      "No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n"
     ]
    }
   ],
   "source": [
    "if not pt.started():\n",
    "    pt.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an index\n",
    "\n",
    "indexer = pt.IterDictIndexer(\n",
    "    \"./indexQVH_path/\",\n",
    "    meta={\n",
    "        \"docno\": 16,\n",
    "        \"vid_name\": 64,\n",
    "        \"text\": 131072,\n",
    "    },\n",
    "    stemmer=\"porter\",\n",
    "    stopwords=\"terrier\",\n",
    "    overwrite=True,\n",
    "    type=pt.index.IndexingType.MEMORY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed = indexer.index(pd.concat([train_documents_set_df, val_documents_set_df]).to_dict('records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BatchRetrieve with the created index and specify BM25 as the weighting model\n",
    "first_stage_bm25 = pt.BatchRetrieve(\n",
    "    indexed,\n",
    "    wmodel=\"BM25\",\n",
    "    num_results=3,\n",
    "    metadata=[\"docno\", \"vid_name\", \"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features, can use any of the features in the list\n",
    "\n",
    "pl2_retriever = pt.BatchRetrieve(indexed, wmodel=\"PL2\")\n",
    "dph_retriever = pt.BatchRetrieve(indexed, wmodel=\"DPH\")\n",
    "#tf_idf_retriever = pt.BatchRetrieve(indexed, wmodel=\"TF_IDF\")\n",
    "#bb2_retriever = pt.BatchRetrieve(indexed, wmodel=\"BB2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a pipeline with the features\n",
    "pipeline_with_features = ~first_stage_bm25 >> (\n",
    "    pl2_retriever ** dph_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the queries for the pipeline, remove special characters and extra spaces\n",
    "prepared_queries = train_query_set_df\n",
    "prepared_queries['qid'] = prepared_queries['qid'].astype(str)\n",
    "prepared_queries['query'] = prepared_queries['query'].str.replace('[\\'\"?!]', ' ', regex=True)\n",
    "prepared_queries['query'] = prepared_queries['query'].str.replace(r'[^\\w\\s]', ' ', regex=True)\n",
    "prepared_queries['query'] = prepared_queries['query'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "\n",
    "prepared_rankings = train_query_rankings_df.reset_index()\n",
    "prepared_rankings['qid'] = prepared_rankings['qid'].astype(str)\n",
    "prepared_rankings['docno'] = prepared_rankings['docno'].astype(str)\n",
    "prepared_rankings['query'] = prepared_rankings['query'].str.replace('[\\'\"?!]', ' ', regex=True)\n",
    "prepared_rankings['query'] = prepared_rankings['query'].str.replace(r'[^\\w\\s]', ' ', regex=True)\n",
    "prepared_rankings['query'] = prepared_rankings['query'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "\n",
    "prepared_rankings['label'] = prepared_rankings['score']\n",
    "\n",
    "prepared_rankings['label'] = prepared_rankings['label'].astype(int)\n",
    "\n",
    "prepared_valqueries = val_query_set_df.reset_index()\n",
    "prepared_valqueries['qid'] = prepared_valqueries['qid'].astype(str)\n",
    "prepared_valqueries['query'] = prepared_valqueries['query'].str.replace('[\\'\"?!]', ' ', regex=True)\n",
    "prepared_valqueries['query'] = prepared_valqueries['query'].str.replace(r'[^\\w\\s]', ' ', regex=True)\n",
    "\n",
    "prepared_valqueries['query'] = prepared_valqueries['query'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "\n",
    "prepared_valrankings = val_query_rankings_df.reset_index()\n",
    "prepared_valrankings['qid'] = prepared_valrankings['qid'].astype(str)\n",
    "prepared_valrankings['docno'] = prepared_valrankings['docno'].astype(str)\n",
    "prepared_valrankings['query'] = prepared_valrankings['query'].str.replace('[\\'\"?!]', ' ', regex=True)\n",
    "prepared_valrankings['query'] = prepared_valrankings['query'].str.replace(r'[^\\w\\s]', ' ', regex=True)\n",
    "prepared_valrankings['query'] = prepared_valrankings['query'].str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "\n",
    "prepared_valrankings['label'] = prepared_valrankings['score']\n",
    "# make the label an int\n",
    "prepared_valrankings['label'] = prepared_valrankings['label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "index = pt.IndexFactory.of(\"./indexQVH_path/\")\n",
    "\n",
    "# load results from csv\n",
    "# results_with_features = pd.read_csv('/content/drive/MyDrive/IR/results_with_features.csv')\n",
    "fsr_pipelines = [\n",
    "    pipeline_with_features\n",
    "]\n",
    "\n",
    "learned_models = [\n",
    "    SVR()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18:44:37.739 [main] WARN org.terrier.querying.ApplyTermPipeline - The index has no termpipelines configuration, and no control configuration is found. Defaulting to global termpipelines configuration of 'Stopwords,PorterStemmer'. Set a termpipelines control to remove this warning.\n",
      "18:44:43.763 [main] WARN org.terrier.querying.ApplyTermPipeline - The index has no termpipelines configuration, and no control configuration is found. Defaulting to global termpipelines configuration of 'Stopwords,PorterStemmer'. Set a termpipelines control to remove this warning.\n",
      "18:44:48.045 [main] WARN org.terrier.querying.ApplyTermPipeline - The index has no termpipelines configuration, and no control configuration is found. Defaulting to global termpipelines configuration of 'Stopwords,PorterStemmer'. Set a termpipelines control to remove this warning.\n"
     ]
    }
   ],
   "source": [
    "trained_models = [first_stage_bm25]\n",
    "names = ['BM25']\n",
    "\n",
    "# ltr_svm = ~pipeline_with_features >> pt.ltr.apply_learned_model(SVR())\n",
    "\n",
    "# ltr_svm.fit(\n",
    "#     prepared_queries,\n",
    "#     train_query_rankings_df,\n",
    "#     # train_documents_set_df\n",
    "# )\n",
    "\n",
    "for fsr_pipeline in fsr_pipelines:\n",
    "    for model in learned_models:\n",
    "        names.append(f\"prova_{model.__class__.__name__}\")\n",
    "        pipe = fsr_pipeline >> pt.ltr.apply_learned_model(model)\n",
    "        pipe.fit(\n",
    "            prepared_queries,\n",
    "            prepared_rankings\n",
    "        )\n",
    "        trained_models.append(pipe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>nDCG@3</th>\n",
       "      <th>RR@3</th>\n",
       "      <th>AP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BM25</td>\n",
       "      <td>0.050780</td>\n",
       "      <td>0.057006</td>\n",
       "      <td>0.046424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BM25 &gt; SVR</td>\n",
       "      <td>0.044434</td>\n",
       "      <td>0.047255</td>\n",
       "      <td>0.037773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         name    nDCG@3      RR@3        AP\n",
       "0        BM25  0.050780  0.057006  0.046424\n",
       "1  BM25 > SVR  0.044434  0.047255  0.037773"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyterrier.measures import nDCG, RR, MAP\n",
    "\n",
    "pt.Experiment(\n",
    "    trained_models,\n",
    "    prepared_valqueries,\n",
    "    prepared_valrankings,\n",
    "    names=['BM25', 'BM25 > SVR'],\n",
    "    eval_metrics=[nDCG @ 3, RR @ 3, MAP],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>nDCG@3</th>\n",
       "      <th>RR@3</th>\n",
       "      <th>AP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BM25</td>\n",
       "      <td>0.056367</td>\n",
       "      <td>0.063330</td>\n",
       "      <td>0.049986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BM25 &gt; SVR</td>\n",
       "      <td>0.050684</td>\n",
       "      <td>0.054443</td>\n",
       "      <td>0.042648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         name    nDCG@3      RR@3        AP\n",
       "0        BM25  0.056367  0.063330  0.049986\n",
       "1  BM25 > SVR  0.050684  0.054443  0.042648"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pt.Experiment(\n",
    "    trained_models,\n",
    "    prepared_queries,\n",
    "    prepared_rankings,\n",
    "    names=['BM25', 'BM25 > SVR'],\n",
    "    eval_metrics=[nDCG @ 3, RR @ 3, MAP],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
